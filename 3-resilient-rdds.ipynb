{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "swedish-migration",
   "metadata": {},
   "source": [
    "# Resilient RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-friend",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-cabin",
   "metadata": {},
   "source": [
    "In the last lessons we saw two of the main components of Spark.  We saw that Spark primarily saves it's data to memory, and that Spark stores a dataset distributed across executors, that query and operate on that data in parallel.  Spark calls this distributed data set a Resilient Distributed Dataset (RDD).\n",
    "\n",
    "Now, storing data distributed across nodes in memory comes with it's challenges -- mainly that if one of the nodes goes down, we'll want a way to recover the lost data, but of course in Spark that data is not saved to disk.  So in this lesson, we'll see how Spark does recover when data when a node goes down.  This should help to explain the *resilient* component of our Resilient Distributed Datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-given",
   "metadata": {},
   "source": [
    "### Building Fault Tolerance In Memory\n",
    "\n",
    "So as we know, the principal feature of Spark is that it provides for in memory storage.  Now in memory storage comes with some challenges -- mainly that even thouh we are not saving any updates to disk, we still want our spark cluster to be *fault tolerant*.  This means that even if one of our nodes goes down, we do not want the data on that node to be lost.\n",
    "\n",
    "Normally, distributed databases achieve this by copying partitions of the data to multiple nodes.  This way if one node goes down, there is still a backup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-implementation",
   "metadata": {},
   "source": [
    "> In the diagram below we can see that the `D` movies are copied to two different nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-auction",
   "metadata": {},
   "source": [
    "<img src=\"./copied_data.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-amino",
   "metadata": {},
   "source": [
    "However, there are downsides to this approach:\n",
    "1. This copying takes up a significant amount of space, and \n",
    "2. It requires copying data over a the cluster's network, and oftentimes there may be narrow bandwidth to do so\n",
    "\n",
    "> The diagram below shows the copying process from one node to the other.  With a lot of data, and narrow bandwidth, this can be a slow process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-senegal",
   "metadata": {},
   "source": [
    "> <img src=\"./network_slow.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-french",
   "metadata": {},
   "source": [
    "In Spark things are done differently.  Instead of copying the data over, from one node to another, Spark instead keeps track of all of the steps to recreate our dataset in the driver node.  So if the node goes down, it can simply reapply those steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-sword",
   "metadata": {},
   "source": [
    "We'll learn more about this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-video",
   "metadata": {},
   "source": [
    "### Getting Setup (On Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-authentication",
   "metadata": {},
   "source": [
    "* Begin by installing some pip packages and the java development kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet\n",
    "!pip install -U -q PyDrive --quiet \n",
    "!apt install openjdk-8-jdk-headless &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-arctic",
   "metadata": {},
   "source": [
    "* Then set the java environmental variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-studio",
   "metadata": {},
   "source": [
    "* Then connect to a SparkSession, setting the spark ui port to `4050`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf(\"spark.ui.port\", \"4050\").setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-plaza",
   "metadata": {},
   "source": [
    "* Then we need to install ngrok which will allow us to place our local spark ui on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statewide-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-abraham",
   "metadata": {},
   "source": [
    "* And finally we get a link our Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-dynamics",
   "metadata": {},
   "source": [
    "### Viewing the Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-supply",
   "metadata": {},
   "source": [
    "Let's get started by using our spark context to create an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "swiss-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['dark knight', 'dunkirk', 'pulp fiction', 'avatar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reliable-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-broadway",
   "metadata": {},
   "source": [
    "This time we'll change our data.  We can do so by using the `map` function to capitalize each word in our list of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indie-clearance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda movie: movie.title()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-chapter",
   "metadata": {},
   "source": [
    "> So above, `movie` represents each title in the list, and we call `title` on each one to capitalize.  We see the results of the transformation with `collect`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-there",
   "metadata": {},
   "source": [
    "* Seeing it in the Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-sheffield",
   "metadata": {},
   "source": [
    "Now we have seen how Spark records these transformations by looking at the Spark UI.  Let's take another look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bigger-surgery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jeffreys-air.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>films</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=films>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-appreciation",
   "metadata": {},
   "source": [
    "> If we click on the Spark UI, and then the most recent Job, we can see the dag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-xerox",
   "metadata": {},
   "source": [
    "> <img src=\"./dag-viz.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-looking",
   "metadata": {},
   "source": [
    "Now, underneath, Spark logs the transformation more in a more detailed manner than the UI illustrates.  But hopefully we can get the idea, that Spark tracks the transformations made on a dataset, from when it's read from an external dataset, to the ultimate output.  And if a node goes down, Spark can re-execute the steps on just that portion of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-update",
   "metadata": {},
   "source": [
    "<img src=\"./rdd_one_to_two.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-popularity",
   "metadata": {},
   "source": [
    "The other thing to note is that when we apply a transformation to a dataset, we are actually creating a new RDD.  Again, we can see that in the DAG.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-planning",
   "metadata": {},
   "source": [
    "> <img src=\"./dag-viz.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-detector",
   "metadata": {},
   "source": [
    "So we are never updating an RDD.  Our RDDs are read only, and whenever we filter or map through a dataset, we are creating a new RDD in the process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-incident",
   "metadata": {},
   "source": [
    "### Only Coarse Transformations\n",
    "\n",
    "Because keeping track of every tiny change that happens to a dataset takes some work, Spark limits the kinds of transformations we can apply.  Namely, when we apply changes, we must apply these changes to the *entire* RDD.  For example, above, we capitalized every record with the `map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "considerable-stations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda movie: movie.title()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-insulin",
   "metadata": {},
   "source": [
    "Or, with our RDDs, we can also go through every record, and only select those that begin with the letter `d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "generic-front",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dark knight', 'dunkirk']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda movie: movie[0] == 'd').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-surrey",
   "metadata": {},
   "source": [
    "> But this is still considered an operation on the entire dataset because we search through every record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-organizer",
   "metadata": {},
   "source": [
    "The point from above, though is that whether we use `map` or `filter`, each step applies to *every* record.  These types of transformations are called **coarse grained transformations** - and these are the only kinds of transformations that Spark allows.  If we were to select individual records and then make changes to them, this would be fine-grained transformations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-advocacy",
   "metadata": {},
   "source": [
    "If we want to apply a transformation to a small subset of data, we'll need to first use a filter to select our matching records, and then apply the map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "binding-discussion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda movie: movie[0] == 'd').map(lambda movie: movie.title()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-gospel",
   "metadata": {},
   "source": [
    "<img src=\"./filter_map.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-empty",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-darwin",
   "metadata": {},
   "source": [
    "So we saw in this lesson that Spark achieves fault tolerance by keeping a recording of the transformations needed to recreate our data.  Because the RDDs are read only, so when we transform our data, really we are creating a new RDD.  And Spark keeps track of the steps necessary to go from one transformation to the other.\n",
    "\n",
    "<img src=\"./rdd_one_to_two.jpg\" width=\"40%\">\n",
    "\n",
    "To make recording these steps easier, on Spark RDDs, we can only apply coarse grained transformations, which apply to the entire Spark dataset.  We'll learn more of these transformations in the following lesson, but to start, `map` which applies the same change to every record, and `filter` which selects from a set of elements are two coarse grained transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-cricket",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Spark Debugging Minibook](https://cs.famaf.unc.edu.ar/~damian/tmp/bib/Mini%20eBook%20-%20Apache%20Spark%20Monitoring%20and%20Debugging.pdf)\n",
    "\n",
    "[Presenting RDDs](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
    "\n",
    "[RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-anthony",
   "metadata": {},
   "source": [
    "* [RDDs Simplified](https://vishnuviswanath.com/spark_rdd)\n",
    "\n",
    "* [Databricks RDDs](https://databricks.com/glossary/what-is-rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-legend",
   "metadata": {},
   "source": [
    "[Databricks best practices](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
